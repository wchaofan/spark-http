status = info
name = sparksql
#Log4j能够自动检测修改配置 文件和重新配置本身，设置间隔秒数
monitorInterval=30

#指定日志文件的位置和文件名称,以便记录多份日志时,直接引用
property.tag.log.dir=logs
property.tag.log.file=tag-server.log

filters = threshold
# 级别过滤（过滤日志记录）
filter.threshold.type = ThresholdFilter
# 只记录debug级别以上的日志，大小写无关：(ALL<)TRACE<DEBUG<INFO<WARN<ERROR<FATAL(<OFF)
filter.threshold.level = info

appenders =console,rolling

appender.console.type = Console
appender.console.name = STDOUT
appender.console.layout.type = PatternLayout
appender.console.layout.pattern =%-d{yyyy-MM-dd HH:mm:ss} %-4r [%t] %-5p %c %x - %m%n
appender.console.filter.threshold.type = ThresholdFilter
appender.console.filter.threshold.level = info


# 文件滚动记录类型的日志输出源
appender.rolling.type = RollingFile
# 指定当前滚动输出源的名称
appender.rolling.name = ROLLING
# 指定当前日志文件的位置和文件名称,可以单独指定,也可以直接引用之前定义过得property.filename参数
appender.rolling.fileName = ${sys:tag.home}/${tag.log.dir}/${tag.log.file}
# 指定当发生Rolling时，文件的转移和重命名规则
appender.rolling.filePattern = ${sys:tag.home}/${tag.log.dir}/${tag.log.file}.%d{yyyy-MM-dd}-%i
# 滚动记录输出源布局类型
appender.rolling.layout.type = PatternLayout
# 滚动记录输出模板
appender.rolling.layout.pattern =%-d{yyyy-MM-dd HH:mm:ss} %-4r [%t] %-5p %c %x - %m%n
# 指定记录文件的封存策略，该策略主要是完成周期性的日志文件封存工作
appender.rolling.policies.type = Policies
# 基于时间的触发策略（TriggeringPolicy）
appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
# 当前记录周期为每2秒生成一个文件，如果filePattern中配置的文件重命名规则是target/rolling2/test1-%d{yyyy-MM-dd HH-mm}-%i，最小的时间粒度是mm，即分钟，TimeBasedTriggeringPolicy指定的size是1，结合起来就是每2分钟生成一个新文件。如果改成%d{yyyy-MM-dd HH}，最小粒度为小时，则每2个小时生成一个文件。
appender.rolling.policies.time.interval =1
# 是否对封存时间进行调制。若modulate=true，则封存时间将以0点为边界进行偏移计算。比如，modulate=true，interval=4hours，那么假设上次封存日志的时间为03:00，则下次封存日志的时间为04:00，之后的封存时间依次为08:00，12:00，16:00
appender.rolling.policies.time.modulate = true
# 基于日志文件体积的触发策略。
appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
# 当日志文件体积大于size指定的值时，触发Rolling
appender.rolling.policies.size.size= 200MB

#控制局部的控制，多level,name 后面可以是包，类甚至可以是多个 以逗号分隔，如果多个logger 在loggers 里面指定多个，以逗号分隔
loggers=file,hadoop
logger.file.name=com.wangchaofan
logger.file.level = info

logger.hadoop.name=org.apache.hadoop
logger.hadoop.level = error
#全局的日志打印设置
rootLogger.level = info
rootLogger.appenderRefs = rolling
rootLogger.appenderRef.rolling.ref = ROLLING

log4j.logger.org.apache.spark.deploy.yarn.SparkRackResolver=ERROR
#appenders = console, file,rolling

# 添加一个新的日志appender
#appender.file.type = File
#appender.file.name = LOGFILE
#appender.file.fileName=${filename}/tag.log.out
#appender.file.layout.type=PatternLayout
#appender.file.layout.pattern=%-d{yyyy-MM-dd HH:mm:ss} %-4r [%t] %-5p %c %x - %m%n
#loggers=file,myfile
#logger.myfile.name=com.wangchaofan
#logger.myfile.level = warn
#后面可以是多个
#logger.myfile.appenderRefs = file
#logger.myfile.appenderRef.file.ref = LOGFILE
